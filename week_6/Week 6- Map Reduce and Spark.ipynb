{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6- MapReduce and Apache Spark\n",
    "\n",
    "**Objectives**: Today we are going to work with Apache Spark on AWS. Spark is one of the most popular \"big data\" platforms and supports both batch processing like Hadoop and streaming. We will also explore the basics of the MapReduce programming model that was the basis for Apache Hadoop. Today we will:\n",
    "  \n",
    "* Review MapReduce\n",
    "* Review the conceptual foundation of MapReduce with parallel Python\n",
    "* Review Spark and its place in the \"big data\" technology ecosystem\n",
    "* Set up our Spark environment including PySpark\n",
    "* Read data from S3 into an RDD\n",
    "* Conduct some analyses using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce and Apache Hadoop\n",
    "\n",
    "The one technology that is most associated with \"big data\" is the MapReduce programming model and its open source framework [Hadoop](https://hadoop.apache.org/). This model uses a functional style to enable computing operations to scale across very large datasets. The functional style of programming enables networked applications to operate over large clusters of independent computers while still maintaining the integrity of the results. MapReduce is designed to work with very large datasets by using clusters that might have thousands of independent, low-cost computers acting as the worker nodes. Unlike traditional systems which required more central capacity in terms of memory or processing (vertical scalability), MapReduce supports a decentralized model that can scale by adding more worker nodes (horizontal scalability). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/map_reduce.png\">\n",
    "(source: http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While MapReduce and Hadoop has been one of hottest analytics technologies over the past five years, there are signs that the technology may be suffering from what Gartner terms [\"the trough of disillusionment\"](http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp).  \n",
    "\n",
    "In December of 2015, the Wall Street Journal said:\n",
    "\n",
    ">Researcher Gartner Inc. says Hadoop adoption [remains low](http://blogs.wsj.com/cio/2015/05/13/hadoop-corporate-adoption-remains-low-gartner/) as firms struggle to articulate Hadoopâ€™s business value and overcome a shortage of workers who have the skills to use it. A survey of 284 global IT and business leaders in May found more than half had no plans to invest in Hadoop. Adoption could grow with the use of [tools based on SQL](http://blogs.wsj.com/cio/2015/03/31/corporate-hadoop-adoption-is-growing-barclays-report-says/), a query language that corporate IT shops know well, Barclays analyst Raimo Lenschow said earlier this year.\n",
    "\n",
    "(source: http://blogs.wsj.com/cio/2015/12/11/cio-explainer-what-is-hadoop/)\n",
    "\n",
    "While firms in general may be struggling to adopt Hadoop and MapReduce, there is little doubt that the programming model continues to be important for large scale data processing especially at Internet firms. Newer technologies like Spark were built on the foundation that MapReduce built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Execution Through One Process\n",
    "\n",
    "To demonstrate the conceptual foundations of MapReduce, we will use TextBlob to count noun phrase using a single-process serial approach and then use the IPython Parallel library to conduct the same analysis using several workers in parallel. We will use the IPython magic command <code>%time</code> to capture the time of execution for each version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This opens the full text of Moby Dick file into a file object\n",
    "\n",
    "\n",
    "import codecs\n",
    "with codecs.open('./datasets/moby_full','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a TextBlob object with the full text of Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we create a TextBlob object from the file \n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "full_text = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the text in the object\n",
    "\n",
    "full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count the noun phrases in the TextBlob using the <code>np_counts</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count noun phrases. Note it will take some time depending on your computer.\n",
    "\n",
    "%time serial = full_text.np_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will derive some statistics from the count we conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Length of noun phrases is {}'.format(len(serial))\n",
    "print 'Sum of noun phrase counts is {}'.format(sum(serial.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Execution Through Four Worker Processes\n",
    "\n",
    "For the next example, we are going to use [IPython Parallel](http://ipyparallel.readthedocs.org/) to conduct the same analysis using a process similar to that of MapReduce. While we will be executing this on our individual computers, the same code with minor changes could be used across different physical devices as a cluster. \n",
    "\n",
    "From command line, start the iPython worker nodes in your working directory:\n",
    "\n",
    "<code>ipcluster start -n 4</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start IPython Parallel in notebook and check for workers\n",
    "\n",
    "from ipyparallel import Client\n",
    "c = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, let's check the client to make sure all four workers have started\n",
    "\n",
    "print 'These are the currently active worker ids:{}'.format(c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use each of these workers in parallel, we need an object that acts as a multiplexer. In IPython Parallel, a DirectView is an object which allows interactive access to these worker processes. In the next code block, we will assign all the workers to a view using a slice approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign all workers to a view\n",
    "\n",
    "dview=c[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this example easier to understand, we split the Moby Dick text into four equal parts which are contained in the datasets folder and have the following names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list = ['moby25a', 'moby25b', 'moby25c', 'moby25d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a DirectView [decorator](http://simeonfranklin.com/blog/2012/jul/1/python-decorators-in-12-steps/) with the @ symbol to make a standard serial function work on a cluster of IPython processes in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def read_texts_parallel(text):\n",
    "    from textblob import TextBlob\n",
    "    import codecs\n",
    "    with codecs.open('./datasets/{}'.format(text[0]),'r',encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "    blob = TextBlob(text)\n",
    "    counts = blob.np_counts\n",
    "    return dict(counts)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a function that:\n",
    "\n",
    "* Maps the individual texts (an iterable list) into the parallel function\n",
    "* Reduces the returns from those functions into a summarized list of noun phrases\n",
    "\n",
    "Counter is a subclass for dictionaries to count values.\n",
    "\n",
    "We will then run the combined map and reduce function using the list of text sections from Moby Dick to compare run times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def map_reduce(texts):\n",
    "    # This effectively maps the iterable list of texts to the function on each worker\n",
    "    mapped_text = read_texts_parallel(texts)\n",
    "    # This takes the returned map results and combines them in the notebook process\n",
    "    reduced = reduce(lambda x, y:Counter(x) + Counter(y), mapped_text)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "%time map_reduced = map_reduce(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the noun phrase counts were the same as the earlier serial process with a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Length of noun phrases is {}'.format(len(map_reduced))\n",
    "print 'Sum of noun phrase counts in {}'.format(sum(map_reduced.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use the powerful [sets module](https://docs.python.org/2/library/sets.html) to compare our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(serial).difference(set(map_reduced))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
