{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (Tuesday)- Streaming Data and Text Analysis\n",
    "\n",
    "**Objectives**: Today we are going to explore streaming data and text analysis. Specifically, we will cover the following:\n",
    "  \n",
    "* Streaming Data\n",
    "* Build code to connect to the Twitter streaming API\n",
    "* Experiment with that code to generate different streams\n",
    "* Covert raw streams into structured data for analysis\n",
    "* Use TextBlob to analyze text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Data\n",
    "\n",
    "Streaming data often embodies all three of the traditional \"big data\" V's- variety, velocity, and volume. Much of this data is also unstructured. In this context, we are discussing streaming data and not streaming media like video or audio although streaming media is also a component of \"big data\".\n",
    "\n",
    "For analytics and streaming data, a typical use case might look like the following example from Amazon Web Services.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/Kinesis-Streams_Diagram.png\">\n",
    "(source: https://aws.amazon.com/kinesis/)\n",
    "\n",
    "A streaming analytics pipeline can be used for many applications including:\n",
    "\n",
    "* real-time machine learning (recommendations, predictions, etc.)\n",
    "* real-time analytics\n",
    "* distributed data generation and collection (JSON activity streams and xAPI)\n",
    "* Internet of Things data collection\n",
    "\n",
    "Open source platforms used to handle and analyze streaming data include:\n",
    "\n",
    "* Apache Kafka for messaging- http://kafka.apache.org/\n",
    "* Apache Storm for real-time computation- http://storm.apache.org/\n",
    "\n",
    "Advanced firms like Palantir are expanding beyond the traditional single stream applications like recommendation to have a system built on analyzing and integrating multiple, disparate data streams as can be seen in these product descriptions:\n",
    "\n",
    "* https://www.palantir.com/solutions/insider-threat/\n",
    "* https://www.palantir.com/palantir-gotham/\n",
    "\n",
    "Financial firms are also using Twitter data directly or from aggregators like RavenPack to either inform or in some cases trigger trades.\n",
    "\n",
    "* http://www.bloomberg.com/bw/articles/2013-04-24/how-many-hft-firms-actually-use-twitter-to-trade\n",
    "* http://www.ravenpack.com/\n",
    "\n",
    "A key tool in the example of financial trading is sentiment analysis. Sentiment analysis uses natural language processing and text processing to infer attitudes about the subject of that text. In simplistic financial terms, if the public has positive attitudes about a given firm or its stock, these attitudes will be correlated with price stability or increases in value. Recent examples have even included April Fools Twitter jokes that have seemed to impact stock prices:\n",
    "\n",
    "* http://blogs.wsj.com/moneybeat/2015/04/01/tesla-stock-moves-on-april-fools-joke/\n",
    "\n",
    "Real-time analysis of the text data associated with these streams was likely involved in trading decisions like this. More broadly, the use of text analysis and streaming data can be a critical component of any firm's analytics efforts.\n",
    "\n",
    "Today we are going to get experience with Twitter's streaming API and basic text analysis of that streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Streaming API\n",
    "\n",
    "Details about the API can be found here:\n",
    "\n",
    "* https://dev.twitter.com/streaming/overview\n",
    "\n",
    "Luckily for us, there is also a Python library which makes access to the API fast and easy:\n",
    "\n",
    "* http://tweepy.readthedocs.org/en/v3.5.0/index.html\n",
    "\n",
    "**Setting up API Credentials**\n",
    "\n",
    "To use the Twitter API, you must first register an application with Twitter in order to get the required access credentials. Go to the following website to create an account and register an application so you can get the credentials that are required to run the code below.\n",
    "\n",
    "* https://dev.twitter.com/\n",
    "\n",
    "Your apps can be managed at:\n",
    "\n",
    "* https://apps.twitter.com/\n",
    "\n",
    "When your application is properly configured, you should be able to access \"Keys and Access Tokens\" in a page that looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/twitter_apps.png\">\n",
    "\n",
    "**Getting Started**\n",
    "\n",
    "To get some experience with the API, we will begin by importing a function we built which makes using Tweepy easier.  Insert your Twitter application credentials into the following code block and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Tweepy functions and include access keys and tokens in global namespace.\n",
    "\n",
    "from tweet_stream import TwitterAuth, PrintStream, FileStream, get_stream\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to build a Python connector to the streaming API with your credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an OAuth object and make a connector\n",
    "\n",
    "auth = TwitterAuth(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "con = auth.make_connector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to use the connector and listener to establish a stream. Streaming is fundementally different from other REST APIs given there is an ongoing HTTP connection that is listening for the stream. The following diagram represents this process for Twitter:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/twitter_streaming-intro-2_1.png\">\n",
    "(source: https://dev.twitter.com/streaming/overview)\n",
    "\n",
    "We have defined a function that is a listener to retrieve the data as it is streamed to your connection.  This listener was built using the <code>PrintStream()</code> class which defines that the results of the stream should be printed to the console.\n",
    "\n",
    "**Print to Console**\n",
    "\n",
    "Once the stream is set up, we are using the <code>filter</code> function to pass an array of search values in the <code>track</code> parameter which the Twitter API uses to pass to the connection. Try entering some search terms in the <code>filter</code> function in the next code block to begin printing output to your notebook.\n",
    "\n",
    "**Please Note:** The data you receive from Twitter may contain inappropriate material, so  use caution if you are sensitive to any particular content.\n",
    "\n",
    "Given this is a streaming connection, the output from the API will continue until you cancel it.  Depending on the volume of your filter terms, the API make take some time before it starts sending you data. To cancel it, use the \"interrupt kernel\" button which is a black square in the Jupyter UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up listener and start stream with defined search terms.\n",
    "\n",
    "listener = PrintStream()\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you may have stopped the console printing, the stream is most likely still active which you can check by calling the stream's <code>running</code> attribute. Do that in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if the stream is still running\n",
    "\n",
    "stream.running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the connection, use the <code>disconnect()</code> function on the stream and then check if it is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Disconnect the stream\n",
    "\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to see if the stream is still active.\n",
    "\n",
    "stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print to File**\n",
    "\n",
    "Next, instead of printing the output of the API to the console, let's write it to a file. To stop the listener, you again have to interrupt the kernel and disconnect the listener. This listener is using a the function we built called <code>FileStream</code> that appends the results from the listener to a filename you specify. Complete the following code then check to see that the file is being created in your file system. Wait until the file reaches a reasonable size and then stop and disconnect the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up listener and start stream with defined search terms.\n",
    "\n",
    "listener = FileStream(filepath='filename')\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data From File**\n",
    "\n",
    "Finally, let's open that file to read the JSON contents into a file as a list of Python dictionaries. We built a small function to make this easier called <code>tweets_list</code>.\n",
    "\n",
    "Each JSON document contains a range of different values most of which can be referenced here:\n",
    "\n",
    "* https://dev.twitter.com/overview/api/tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def tweets_list(filename):\n",
    "    \"\"\"\n",
    "    Read lines from filepath and file into a list of dictionaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    return tweets     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code above to read the <code>tweets_list</code> function into the namespace.  Then, use the function to read your tweets file into a list of dictionaries and assign it to a variable so you can explore it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read your tweet file with tweets_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>tweets_list</code> returns a list of the JSON documents in dictionary format from the file you specified. Use Python's list index to explore different tweets.  Within individual tweets, use Python's dictionary key-value access method to explore specific data. The Pretty Print module is useful in situations like this as it will better format print output.\n",
    "\n",
    "* https://docs.python.org/2/library/pprint.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore tweet data structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use pandas to read the tweets into a dataframe in the next codeblock. Each document may have a different number of first level keys, so pandas will use the max number of keys to define columns and then apply NaN (or null values) in those cases where the keys are not present.\n",
    "\n",
    "The general syntax to accomplish this is:\n",
    "\n",
    "```\n",
    "dataframe = pandas.DataFrame(list of Python dictionaries)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe from list of dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine the dataframe to get a sense for what elements are included in it. Be aware that the Twitter JSON was nested, so there will be elements in the dataframe that are of type <code>dict</code>. Use slicing and indexing to evaluate individual elements.\n",
    "\n",
    "One interesting example of nesting is the <code>retweeted_status</code> which is a complete representation of the original tweet that was retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) and Text Analysis\n",
    "\n",
    "The analysis of unstructured text data like that in tweets is the domain of natural language processing (NLP). NLP is a significant area of study for computer scientists both in academia and industry. These efforts include dedicated research groups like:\n",
    "\n",
    "* **Stanford NLP Group:** http://nlp.stanford.edu/\n",
    "* **Google NLP:** http://research.google.com/pubs/NaturalLanguageProcessing.html\n",
    "* **Microsoft NLP:** http://research.microsoft.com/en-us/groups/nlp/\n",
    "\n",
    "In this lab, we are going to provide a very basic introduction to text analysis using the TextBlog Python Library:\n",
    "\n",
    "* https://textblob.readthedocs.org/en/dev/\n",
    "\n",
    "TextBlob offers a simple interface for experimenting with NLP. It relies on the more robust Python Natural Language Toolkit or NLTK.\n",
    "\n",
    "* http://www.nltk.org/\n",
    "\n",
    "In addition to the code for conducting NLP analyses, you typically also need a corpus or dataset which provides the equivilent of metadata for text data.  For example, to classify a word as a \"noun\" requires a dataset with that word and the word's part of speech which can be looked up based on the word itself and potentially its context within the text. So, for our purposes, we also need to install the NLTK corpora.\n",
    "\n",
    "* http://www.nltk.org/data.html\n",
    "\n",
    "**Read Some Text into Memory**\n",
    "\n",
    "To experiment with TextBlob, we will start with opening the \"alice.txt\" file from the \"datasets\" folder in this directory. We will then assign the contents of the file to a variable in our namespace. Run the code in the next text block to do that.\n",
    "\n",
    "\n",
    "> A quick aside: Python 2.x can be challenging to work with text data given Python type <code>str</code> doesn't explicitly support unicode.  This blog post is a good summary of the issue- http://www.azavea.com/blogs/labs/2014/03/solving-unicode-problems-in-python-2-7/. The code below uses the codecs module to create a <code>unicode</code> type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a file object and read its contents into a variable as unicode.\n",
    "\n",
    "import codecs\n",
    "with codecs.open('datasets/alice.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the type of the text object here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a TextBlob Object and Derive Basic Statistics**\n",
    "\n",
    "Now, we will import TextBlob and create an object from our text variable that we can use to try several different NLP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob has an intuitive API which makes basic text analysis quite easy. It can also be used to create custom text classifiers. \n",
    "\n",
    "* **The Basics:** https://textblob.readthedocs.org/en/dev/quickstart.html\n",
    "* **Building a Text Classifier:** https://textblob.readthedocs.org/en/dev/classifiers.html\n",
    "\n",
    "Try some of the basics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a word count\n",
    "\n",
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out noun phrases\n",
    "\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count noun phrases\n",
    "\n",
    "blob.np_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Derive https://textblob.readthedocs.org/en/dev/api_reference.html#textblob.blob.TextBlob.sentiment\n",
    "\n",
    "blob.sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
