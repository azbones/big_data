{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (Tuesday)- Streaming Data and Text Analysis\n",
    "\n",
    "**Objectives**: Today we are going to explore streaming data and text analysis. Specifically, we will cover the following:\n",
    "  \n",
    "* Streaming Data\n",
    "* Build code to connect to the Twitter streaming API\n",
    "* Experiment with that code to generate different streams\n",
    "* Use TextBlob to analyze results from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Streaming Data\n",
    "\n",
    "Streaming data often embodies all three of the \"big data\" V's- variety, velocity, and volume. Much of this data is also unstructured. In this context, we are discussing streaming data and not streaming media like video or audio although streaming media is also a component of \"big data\".\n",
    "\n",
    "For analytics and streaming data, a typical use case might look like the following example from Amazon Web Services.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/Kinesis-Streams_Diagram.png\">\n",
    "(source: https://aws.amazon.com/kinesis/)\n",
    "\n",
    "A streaming analytics pipeline can be used for many applications including:\n",
    "\n",
    "* real-time machine learning (recommendations, predictions, etc.)\n",
    "* real-time analytics\n",
    "* distributed data generation and collection (JSON activity streams and xAPI)\n",
    "* Internet of Things data collection\n",
    "\n",
    "Open source platforms used to handle and analyze streaming data include:\n",
    "\n",
    "* Apache Kafka for messaging- http://kafka.apache.org/\n",
    "* Apache Storm for real-time computation- http://storm.apache.org/\n",
    "\n",
    "Advanced firms like Palantir are expanding beyond the traditional single stream applications like recommendation to have a system built on analyzing and integrating multiple, desparate data streams as can be seen in these product descriptions:\n",
    "\n",
    "* https://www.palantir.com/solutions/insider-threat/\n",
    "* https://www.palantir.com/palantir-gotham/\n",
    "\n",
    "Financial firms are also using Twitter data directly or from aggregators like RavenPack to either inform or is some cases trigger trades.\n",
    "\n",
    "* http://www.bloomberg.com/bw/articles/2013-04-24/how-many-hft-firms-actually-use-twitter-to-trade\n",
    "* http://www.ravenpack.com/\n",
    "\n",
    "A key tool in the example of financial trading is sentiment analysis. Sentiment analysis uses natural language processing and text processing to infer attitudes about the subject of that text. In simplistic financial terms, if the public has positive attitudes about a given firm or its stock, that is usually coorelated with price stability or increases. Recent examples have even included April Fools Twitter jokes that have seemed to impact stock prices:\n",
    "\n",
    "* http://blogs.wsj.com/moneybeat/2015/04/01/tesla-stock-moves-on-april-fools-joke/\n",
    "\n",
    "Real-time analysis of the text data associated with these streams was likely involved in trading decisions like this. More broadly, the use of text analysis and streaming data can be a critical component of any firms analytics efforts.\n",
    "\n",
    "Today we are going to get experience with Twitter's streaming API and basic text analysis of that streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Twitter Streaming API\n",
    "\n",
    "Details about the API can be found here:\n",
    "\n",
    "* https://dev.twitter.com/streaming/overview\n",
    "\n",
    "Luckily for us, there is also a Python library which makes access to the API fast and easy:\n",
    "\n",
    "* http://tweepy.readthedocs.org/en/v3.5.0/index.html\n",
    "\n",
    "**Setting up API Credentials**\n",
    "\n",
    "To use the Twitter API, you must first register an application with Twitter in order to get the required access credentials. Go to the following website to create an account and register an application so you can get the credentials that are required to run the code below.\n",
    "\n",
    "* https://dev.twitter.com/\n",
    "\n",
    "Your apps can be managed at:\n",
    "\n",
    "* https://apps.twitter.com/\n",
    "\n",
    "When your application is properly configured, you should be able to access \"Keys and Access Tokens\" in a page that looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/twitter_apps.png\">\n",
    "\n",
    "**Getting Started**\n",
    "\n",
    "To get some experience with the API, we will begin by importing a function we built which makes using Tweepy easier.  Insert your Twitter application credentials into the following code block and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Tweepy functions and include access keys and tokens in global namespace.\n",
    "\n",
    "from tweet_stream import TwitterAuth, PrintStream, FileStream, get_stream\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to build a Python connector to the streaming API with your credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an OAuth object and make a connector\n",
    "\n",
    "auth = TwitterAuth(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "con = auth.make_connector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to use the connector and listener to establish a stream. Streaming is fundementally different from other REST APIs given there is an ongoing HTTP connection that is listening for the stream. The following diagram represents this process for Twitter:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/twitter_streaming-intro-2_1.png\">\n",
    "(source: https://dev.twitter.com/streaming/overview)\n",
    "\n",
    "We have defined a function that is a listener to retrieve the data as it is streamed to your connection.  This listener was built using the <code>PrintStream()</code> class which defines that the results of the stream should be printed to the console.\n",
    "\n",
    "Once the stream is set up, we are using the <code>filter</code> function to pass an array of search values in the <code>track</code> parameter which the Twitter API uses to pass to the connection.\n",
    "\n",
    "Given this is a streaming connection, the output from the API will continue until you cancel it.  To cancel it, use the \"interrupt kernel\" button which is a black square in the Jupyter UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up listener and start stream with defined search terms.\n",
    "\n",
    "listener = PrintStream()\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=['Broncos','Cardinals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you may have stopped the console printing, the stream is most likely still active which you can check by calling the stream's <code>running</code> attribute. Do that in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if the stream is still running\n",
    "\n",
    "stream.running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the connection, use the <code>disconnect()</code> function on the stream and then check if it is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Disconnect the stream\n",
    "\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to see if the stream is still active.\n",
    "\n",
    "stream.running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instead of printing the output of the API to the console, let's write it to a file. To stop the listener, you again have to interrupt the kernel and disconnect the listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up listener and start stream with defined search terms.\n",
    "\n",
    "listener = FileStream(filepath='naruto.txt')\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=['Naruto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's open that file to read the JSON contents into a file as a Python dictionary. We build a small function to make this easier called <code>tweets_list</code>.\n",
    "\n",
    "Each JSON document contains a range of different values most of which can be referenced here:\n",
    "\n",
    "* https://dev.twitter.com/overview/api/tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def tweets_list(filename):\n",
    "    \"\"\"\n",
    "    Read lines from file into a list of dictionaries.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    return tweets     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lists = tweets_list('naruto.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(lists)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_me=df.ix[26]['retweeted_status']\n",
    "\n",
    "org_tweets=df[df['retweeted_status'].isnull()]\n",
    "\n",
    "df['text'].value_counts()\n",
    "org_tweets.ix[0:1000]['text']\n",
    "org_tweets_blob=''.join(org_tweets)\n",
    "org_list=org_tweets['text'][~org_tweets['text'].isnull()].tolist()\n",
    "org_list=org_tweets['text'].tolist()\n",
    "text_blob = ''.join(org_list)\n",
    "df['text'][1]\n",
    "text_blob[:3000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
