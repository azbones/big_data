{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 (Tuesday)- Streaming Data and Text Analysis\n",
    "\n",
    "**Objectives**: Today we are going to explore streaming data and text analysis. Specifically, we will cover the following:\n",
    "  \n",
    "* Streaming Data\n",
    "* Build code to connect to the Twitter streaming API\n",
    "* Experiment with that code to generate different streams\n",
    "* Use TextBlob to analyze results from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Streaming Data\n",
    "\n",
    "Streaming data often embodies all three of the \"big data\" V's- variety, velocity, and volume. Much of this data is also unstructured. In this context, we are discussing streaming data and not streaming media like video or audio although streaming media is also a component of \"big data\".\n",
    "\n",
    "For analytics and streaming data, a typical use case might look like the following example from Amazon Web Services.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/Kinesis-Streams_Diagram.png\">\n",
    "(source: https://aws.amazon.com/kinesis/)\n",
    "\n",
    "A streaming analytics pipeline can be used for many applications including:\n",
    "\n",
    "* real-time machine learning (recommendations, predictions, etc.)\n",
    "* real-time analytics\n",
    "* distributed data generation and collection (JSON activity streams and xAPI)\n",
    "* Internet of Things data collection\n",
    "\n",
    "Open source platforms used to handle and analyze streaming data include:\n",
    "\n",
    "* Apache Kafka for messaging- http://kafka.apache.org/\n",
    "* Apache Storm for real-time computation- http://storm.apache.org/\n",
    "\n",
    "Advanced firms like Palantir are expanding beyond the traditional single stream applications like recommendation to have a system built on analyzing and integrating multiple, desparate data streams as can be seen in these product descriptions:\n",
    "\n",
    "* https://www.palantir.com/solutions/insider-threat/\n",
    "* https://www.palantir.com/palantir-gotham/\n",
    "\n",
    "Financial firms are also using Twitter data directly or from aggregators like RavenPack to either inform or is some cases trigger trades.\n",
    "\n",
    "* http://www.bloomberg.com/bw/articles/2013-04-24/how-many-hft-firms-actually-use-twitter-to-trade\n",
    "* http://www.ravenpack.com/\n",
    "\n",
    "A key tool in the example of financial trading is sentiment analysis. Sentiment analysis uses natural language processing and text processing to infer attitudes about the subject of that text. In simplistic financial terms, if the public has positive attitudes about a given firm or its stock, that is usually coorelated with price stability or increases. Recent examples have even included April Fools Twitter jokes that have seemed to impact stock prices:\n",
    "\n",
    "* http://blogs.wsj.com/moneybeat/2015/04/01/tesla-stock-moves-on-april-fools-joke/\n",
    "\n",
    "Real-time analysis of the text data associated with these streams was likely involved in trading decisions like this. More broadly, the use of text analysis and streaming data can be a critical component of any firms analytics efforts.\n",
    "\n",
    "Today we are going to get experience with Twitter's streaming API and basic text analysis of that streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Twitter Streaming API\n",
    "\n",
    "Details about the API can be found here:\n",
    "\n",
    "* https://dev.twitter.com/streaming/overview\n",
    "\n",
    "Luckily for us, there is also a Python library which makes access to the API fast and easy:\n",
    "\n",
    "* http://tweepy.readthedocs.org/en/v3.5.0/index.html\n",
    "\n",
    "**Setting up API Credentials**\n",
    "\n",
    "To use the Twitter API, you must first register an application with Twitter in order to get the required access credentials. Go to the following website to create an account and register an application so you can get the credentials that are required to run the code below.\n",
    "\n",
    "* https://dev.twitter.com/\n",
    "\n",
    "Your apps can be managed at:\n",
    "\n",
    "* https://apps.twitter.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweet_stream import TwitterAuth, PrintStream, FileStream, get_stream\n",
    "\n",
    "# consumer_key = 'insert_here'\n",
    "# consumer_secret = 'insert_here'\n",
    "# access_token = 'insert_here'\n",
    "# access_token_secret = 'insert_here'\n",
    "\n",
    "consumer_key = 'insert_here'\n",
    "consumer_secret = 'insert_here'\n",
    "access_token = 'insert_here'\n",
    "access_token_secret = 'insert_here'\n",
    "\n",
    "auth = TwitterAuth(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "con = auth.make_connector()\n",
    "listener = PrintStream()\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=['Broncos','Cardinals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tweets = []\n",
    "f = open('tweets.txt', 'r')\n",
    "for line in f:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "test_me=df.ix[26]['retweeted_status']\n",
    "\n",
    "org_tweets=df[df['retweeted_status'].isnull()]\n",
    "\n",
    "df['text'].value_counts()\n",
    "org_tweets.ix[0:1000]['text']\n",
    "org_tweets_blob=''.join(org_tweets)\n",
    "org_list=org_tweets['text'][~org_tweets['text'].isnull()].tolist()\n",
    "org_list=org_tweets['text'].tolist()\n",
    "text_blob = ''.join(org_list)\n",
    "df['text'][1]\n",
    "text_blob[:3000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
