{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3- Cloud Based Analysis Technologies and External Data Sources\n",
    "\n",
    "**Objectives**: Today we are going to discuss the category of cloud-based analytics tools and extend our Python workflow to work with one example of such tools. We will also work with a streaming data source. Specifcally, we will cover the following:\n",
    "  \n",
    "* The Larger Ecosystem of Big Data Technologies\n",
    "* PaaS Analytics Tools\n",
    "* Google BigQuery\n",
    "* pandas and BigQuery\n",
    "* More dataframe operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Larger Ecosystem of Big Data Technologies\n",
    "\n",
    "**Analytics PaaS Products**\n",
    "\n",
    "While Python on its own is an important tool of data science, the nature of \"big data\" requires a multi-technology approach in most cases. \n",
    "\n",
    "Early discussions of the technology of \"big data\" typically revolved around Hadoop and MapReduce which were some of the first tools that could handle Internet-scale data sources. More recently, a whole variety of different technologies have emerged both in response to not only the larger scale, but also the increased focus on \"analytics 3.0\" applications. This week we will explore the general category of cloud-based analytics technologies that usually fall into the Platform as a Service (PaaS) category. These cloud offerings enable firms to outsource the management of various \"big data\" functions to technology firms both large and small. [Amazon (Amazon Web Services](https://aws.amazon.com/big-data/), [Google (Cloud Platform)](https://cloud.google.com/solutions/bigdata/), and [Microsoft (Azure)](https://azure.microsoft.com/en-us/blog/topics/big-data/) all have extensive PaaS \"big data\" offerings. More specialist providers like [HortonWorks](http://hortonworks.com/) and [Databricks](https://databricks.com/) are even hoping to make the entire process of data science accessble to their customers. Databricks, for example, describes their product as:\n",
    "\n",
    ">Data science made easy, from ingest to production.\n",
    ">We believe big data should be simple. \n",
    ">Apache Spark™ made a big step towards this goal. Databricks makes Spark easy through a cloud-based integrated workspace. (https://databricks.com/product/databricks - Nov 2015)\n",
    "\n",
    "**Python in the PaaS Ecosystem**\n",
    "\n",
    "While Python is a strng technology contender in desktop and server-based data science, it is also being used in these PaaS products as both an underlying technical foundation and as a common data science API. Two examples of this include Amazon's Redshift which now has user [defined functions (UDFs) that are written in Python](https://aws.amazon.com/blogs/aws/user-defined-functions-for-amazon-redshift/) and [Apache Spark which has a robust Python API](http://spark.apache.org/docs/latest/api/python/).\n",
    "\n",
    "Jupyter Notebooks like this one are also part of the products of \"big data\" offerings of Databricks, Google, and Amazon:\n",
    "\n",
    "* https://databricks.com/product/databricks#notebooks\n",
    "* https://cloud.google.com/datalab/\n",
    "* https://blogs.aws.amazon.com/bigdata/post/TxX4BY5T1PQ7BQ/Using-IPython-Notebook-to-Analyze-Data-with-Amazon-EMR\n",
    "\n",
    "Today, we are going to add Google's BigQuery analytics platform to our workflow as an exemplar of the broader category of \"big data\" PaaS offerings. We will begin with a short description of BigQuery and then move on to working with some of the public datasets on BigQuery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BigQuery\n",
    "\n",
    "Google BigQuery is the \"productization\" of the technology that was code named \"Dremel\" at Google. In their 2010 whitepaper, Google described Dremel as:\n",
    "\n",
    ">Dremel is a scalable, interactive ad-hoc query system for analysis\n",
    "of read-only nested data. By combining multi-level execution\n",
    "trees and columnar data layout, it is capable of running aggregation\n",
    "queries over trillion-row tables in seconds. The system scales\n",
    "to thousands of CPUs and petabytes of data, and has thousands\n",
    "of users at Google. (http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf - Nov 2015)\n",
    "\n",
    "The whitepaper gave the following examples of how Google has been using Dremel since 2006:\n",
    "\n",
    ">* Analysis of crawled web documents.\n",
    ">* Tracking install data for applications on Android Market.\n",
    ">* Crash reporting for Google products.\n",
    ">* OCR results from Google Books.\n",
    ">* Spam analysis.\n",
    ">* Debugging of map tiles on Google Maps.\n",
    ">* Tablet migrations in managed Bigtable instances.\n",
    ">* Results of tests run on Google’s distributed build system.\n",
    ">* Disk I/O statistics for hundreds of thousands of disks.\n",
    ">* Resource monitoring for jobs run in Google’s data centers.\n",
    ">* Symbols and dependencies in Google’s codebase.\n",
    "\n",
    "At the high level, by focusing on a read-only and columnar data structure instead of a traditional realational database, Google was able to achieve high scale and good interactive performance. Compared to MapReduce models which are batch based, the technology behind Dremel could enable an interactive data science workflow.   \n",
    "\n",
    "Following the Amazon model of turning internal technolgies into PaaS offerings, Google launched BigQuery as a product in 2010. They describe the product as:\n",
    "\n",
    ">BigQuery is Google's fully managed, NoOps, low cost data analytics service. With BigQuery you have no infrastructure to manage and don't need a database administrator, use familiar SQL and can take advantage of pay-as-you-go model. This collection of features allows you to focus on analyzing data to find meaningful insights. BigQuery is a powerful Big Data analytics platform used by all types of organizations, from startups to Fortune 500 companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIgning up for Google BigQuery\n",
    "\n",
    "One of the advantages of BigQuery for data science is that it has a web UI, so you can explore data from a web interface instead of having to use a SQL client or API. In our case, this will allow you to test your SQL result prior to importing the results into Python for more detailed analysis.\n",
    "\n",
    "For our class, BigQuery is also convenient in that Google has several different publicly datasets that you can access once you set up an account. These public dataset are maintained by Google's Felipe Hoffa details of which are here:\n",
    "\n",
    "* https://www.reddit.com/r/bigquery/wiki/datasets\n",
    "\n",
    "\n",
    "Google has a free trail that includes a $300 credit that can be used over 60 days. BigQuery also has a free usage tier up to 1 TB of data processed per month, so if you are using the public datasets (which have no storage costs) and only doing exploratory analysis, it is unlikely you will incur any charges using BigQuery in this course. \n",
    "\n",
    "My actual costs running the exercises in these notebooks were $XXXX.\n",
    "\n",
    "The BigQuery console (https://console.developers.google.com/billing) will allow you to track your usage to ensure you don't incur any charges. While the trial credits and free usage tier *should* be more than enough to allow you to complete all the exercises in this course.\n",
    "\n",
    "**IF YOU EXCEED THESE LIMITS THEY WILL CHARGE YOUR CREDIT CARD. YOU ARE RESPONSIBLE FOR MANAGING YOUR GOOGLE USAGE.** \n",
    "\n",
    "If you have any concerns about this, please let me know and I can try to accomodate you.   \n",
    "\n",
    "You can either sign up with your ASU Google account or a personal Google Account. To sign up from ASU, login to your ASU account and visit this page which includes all of the relevant service details:\n",
    "\n",
    "https://cloud.google.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BigQuery UI\n",
    "\n",
    "Once you have signed up for the service and area logged into the BigQuery console, you can add the public datasets to your console by pasting their URLs into your browser.  For example, paste or click on the following links after you are logged into the Google Cloud and they should be added to your available projects:\n",
    "\n",
    "* https://bigquery.cloud.google.com/table/bigquery-samples:reddit.full\n",
    "* https://bigquery.cloud.google.com/dataset/imjasonh-storage:nfl\n",
    "\n",
    "The console should look like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/week3-gbq_console.png\">\n",
    "\n",
    "From the console, you can select a dataset from the left nav which will then make the query box active. In the example below, I have selected the [GSOD dataset which contains weather data from NOAA](https://data.noaa.gov/dataset/global-surface-summary-of-the-day-gsod). Under the query box, there are options to view the table schema and details of the dataset.\n",
    "\n",
    "The query screen looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/week3-gbq_query.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Syntax\n",
    "\n",
    "Google's BigQuery products uses SQL-like syntax, but does not conform to ANSI SQL. Details of the syntax are included here:\n",
    "\n",
    "* https://cloud.google.com/bigquery/query-reference\n",
    "\n",
    "To get some experience in BigQuery, use the UI to derive the following metrics using the the defined BigQuery command. For this exercise, we are not running code in the notebook, so just check your own work.\n",
    "\n",
    "Use the GSOD weather dataset:\n",
    "\n",
    "* https://bigquery.cloud.google.com/table/publicdata:samples.gsod\n",
    "\n",
    "**BigQuery Exercises**\n",
    "\n",
    "All of these statements begin with <code>SELECT</code> and build out criteria to define the selection.\n",
    "\n",
    "* Use <code>COUNT</code> and <code>GROUP BY</code> to calculate the number of observations in the year 1989\n",
    "\n",
    "* Use <code>WHERE</code> to return all of the observations with mean wind speeds greater than 75\n",
    "\n",
    "* Use <code>COUNT</code> and <code>WHERE</code> to return the number of observations with mean wind speeds greater than 75\n",
    "\n",
    "* Use <code>AVG</code>, <code>STDDEV</code>, and <code>AS</code> to calculate the average of the mean temperatures across all observations and to give the results a descriptive name\n",
    "\n",
    "* Use <code>AVG</code>, <code>STDDEV</code>, and <code>AS</code> to calculate the average of the mean temperatures across all observations and to give the results a descriptive name\n",
    "\n",
    "**BigQuery Table Organization**\n",
    "\n",
    "A common model for sending data to BigQuery splits tables into different time intervals. Querying across such intervals is easy as multiple tables with the same schema can be added to the <code>FROM</code> statement separated by a comma. For example, Google Analytics Premium customers can get raw, hit level data exported to BigQuery. The table naming convention for this appends the specific date of the extract to the table name like this:\n",
    "\n",
    "<code>dataset.ga_sessions_20151115</code>\n",
    "\n",
    "BigQuery then allows queries that define a time-based wildcard to retrieve specific date ranges without a scan of all tables like:\n",
    "\n",
    "````\n",
    "SELECT COUNT(*)\n",
    "FROM (TABLE_DATE_RANGE(dataset.ga_sessions_, \n",
    "      TIMESTAMP('2015-11-01'), \n",
    "      TIMESTAMP('2015-11-15'))) \n",
    "WHERE device.browser = \"Chrome\"\n",
    "````\n",
    "\n",
    "With ongoing data extracts that are time based, such organization can further optimize performance with multi-terrabyte sized datasets given research questions may be limited by time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas and BigQuery\n",
    "\n",
    "The pandas library has a submodule which offers direct access to Google's BigQuery for read and write access called <code>gbq</code>. For desktop analysis, this submodule works by using [OAuth2.0(http://oauth.net/2/) to authenticate the desktop application as a valid user of the BigQuery account. Practically, you accomplish this by being logged into BigQuery in your default browser and then running the the <code>gbq</code> method.\n",
    "\n",
    "The basic read syntax follows:\n",
    "\n",
    "```\n",
    "projectid = \"xxxxxxxx\"\n",
    "\n",
    "df = pd.read_gbq('SELECT * FROM test_dataset.test_table', projectid)\n",
    "```\n",
    "\n",
    "When populated with a valid project id from the BigQuery console, the <code>read_gbq</code> method will open the default browser to an authentication page for BigQuery and, if successful, then save an access token to the local file system to provide access in the future. Once access is granted, <code>read_gbq</code> will send the SQL command in parameter <code>query</code> to BigQuery and return the results. The method's parameters include:\n",
    "\n",
    "\n",
    "Parameters:\t\n",
    "* query : str : SQL-Like Query to return data values\n",
    "* project_id : str : Google BigQuery Account project ID.\n",
    "* index_col : str (optional) : Name of result column to use for index in results DataFrame\n",
    "* col_order : list(str) (optional) : List of BigQuery column names in the desired order for results DataFrame\n",
    "* reauth : boolean (default False) : Force Google BigQuery to reauthenticate the user. This is useful if multiple accounts are used.\n",
    "* verbose : boolean (default True) : Verbose output\n",
    "\n",
    "An example of code and the output from a first authentication run of the code is in the code and markdown blocks below. Note that the <code>%time</code> magic command entered before a statement in IPython returns various time information realted to its execution. This information can be useful especially when calling an external API like BigQuery to measure performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io import gbq\n",
    "\n",
    "query = \"\"\"SELECT count(*)\n",
    "           FROM publicdata:samples.gsod\"\"\"\n",
    "\n",
    "project_id = XXXXXXXXX\n",
    "\n",
    "%time gsod_year = gbq.read_gbq(query, project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output From First Run:\n",
    "\n",
    "```\n",
    "Your browser has been opened to visit:\n",
    "\n",
    "    https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&response_type=code&client_id=XXXXXX.apps.googleusercontent.com&access_type=offline\n",
    "\n",
    "If your browser is on a different machine then exit and re-run this\n",
    "application with the command-line parameter \n",
    "\n",
    "  --noauth_local_webserver\n",
    "\n",
    "Authentication successful.\n",
    "Job not yet complete...\n",
    "CPU times: user 183 ms, sys: 1.36 s, total: 1.54 s\n",
    "Wall time: 14.4 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweet_stream import TwitterAuth, PrintStream, FileStream, get_stream\n",
    "\n",
    "# consumer_key = 'insert_here'\n",
    "# consumer_secret = 'insert_here'\n",
    "# access_token = 'insert_here'\n",
    "# access_token_secret = 'insert_here'\n",
    "\n",
    "consumer_key = 'insert_here'\n",
    "consumer_secret = 'insert_here'\n",
    "access_token = 'insert_here'\n",
    "access_token_secret = 'insert_here'\n",
    "\n",
    "auth = TwitterAuth(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "con = auth.make_connector()\n",
    "listener = PrintStream()\n",
    "stream = get_stream(con, listener)\n",
    "stream.filter(track=['Broncos','Cardinals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tweets = []\n",
    "f = open('tweets.txt', 'r')\n",
    "for line in f:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_me=df.ix[26]['retweeted_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "org_tweets=df[df['retweeted_status'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "org_tweets.ix[0:1000]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_tweets_blob=''.join(org_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "org_list=org_tweets['text'][~org_tweets['text'].isnull()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "org_list=org_tweets['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_blob = ''.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_blob[:3000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
