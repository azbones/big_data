{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3- Cloud Based Analysis Technologies and External Data Sources\n",
    "\n",
    "**Objectives**: Today we are going to explore the category of cloud-based analytics tools and extend our Python workflow to work with one example of such tools. Specifically, we will cover the following:\n",
    "  \n",
    "* The Larger Ecosystem of Big Data Technologies\n",
    "* PaaS Analytics Tools\n",
    "* Google BigQuery\n",
    "* pandas and BigQuery\n",
    "* More dataframe operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Larger Ecosystem of Big Data Technologies\n",
    "\n",
    "**Analytics PaaS Products**\n",
    "\n",
    "While Python on its own is an important tool of data science, the nature of \"big data\" requires a multi-technology approach in most cases. \n",
    "\n",
    "Early discussions of the technology of \"big data\" typically revolved around Hadoop and MapReduce given they were some of the first tools that could handle Internet-scale data sources. These technologies had the ability to scale horizontally (increasing capacity by adding more computing devices or nodes) compared to traditional approaches that required vertical scaling (increasing capacity by adding processing power to a single computer or cluster). Horizontal scaling meant firms like Google and Yahoo! could use low-cost, commodity hardware to build out systems that could handle massive amounts of data via parallel processing. Not only were such systems more cost effective than the traditional super-computing approaches, but they could support much, much higher scale. We will discuss Map Reduce in more detail during week 6.\n",
    "\n",
    "More recently, a whole variety of different technologies have emerged both in response to not only the larger scale, but also the increased focus on \"analytics 3.0\" applications. This week we will explore the general category of cloud-based analytics technologies that usually fall into the Platform as a Service (PaaS) category. These cloud offerings enable firms to outsource the management of various \"big data\" functions to technology firms both large and small. [Amazon (Amazon Web Services](https://aws.amazon.com/big-data/), [Google (Cloud Platform)](https://cloud.google.com/solutions/bigdata/), and [Microsoft (Azure)](https://azure.microsoft.com/en-us/blog/topics/big-data/) all have extensive PaaS \"big data\" offerings. More specialist providers like [HortonWorks](http://hortonworks.com/) and [Databricks](https://databricks.com/) are even hoping to make the entire process of data science accessible to their customers, not just the analysis tools. Databricks, for example, describes their product as:\n",
    "\n",
    ">Data science made easy, from ingest to production.\n",
    ">We believe big data should be simple. \n",
    ">Apache Spark™ made a big step towards this goal. Databricks makes Spark easy through a cloud-based integrated workspace. (https://databricks.com/product/databricks - Nov 2015)\n",
    "\n",
    "**Python in the PaaS Ecosystem**\n",
    "\n",
    "While Python is a strong technology contender in desktop and server-based data science, it is also being used in these PaaS products as both an underlying technical foundation and as a common data science API. Two examples of this include Amazon's Redshift which now has [user defined functions (UDFs) that are written in Python](https://aws.amazon.com/blogs/aws/user-defined-functions-for-amazon-redshift/) and [Apache Spark which has a robust Python API](http://spark.apache.org/docs/latest/api/python/).\n",
    "\n",
    "Jupyter Notebooks like this one are also part of the products of \"big data\" offerings of Databricks, Google, and Amazon:\n",
    "\n",
    "* https://databricks.com/product/databricks#notebooks\n",
    "* https://cloud.google.com/datalab/\n",
    "* https://blogs.aws.amazon.com/bigdata/post/TxX4BY5T1PQ7BQ/Using-IPython-Notebook-to-Analyze-Data-with-Amazon-EMR\n",
    "\n",
    "Today, we are going to add Google's BigQuery analytics platform to our workflow as an exemplar of the broader category of \"big data\" PaaS offerings. We will begin with a short description of BigQuery and then move on to working with some of the public datasets on BigQuery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google BigQuery\n",
    "\n",
    "Google BigQuery is the \"productization\" of the technology that was code named \"Dremel\" at Google. In their 2010 whitepaper, Google described Dremel as:\n",
    "\n",
    ">Dremel is a scalable, interactive ad-hoc query system for analysis\n",
    "of read-only nested data. By combining multi-level execution\n",
    "trees and columnar data layout, it is capable of running aggregation\n",
    "queries over trillion-row tables in seconds. The system scales\n",
    "to thousands of CPUs and petabytes of data, and has thousands\n",
    "of users at Google. (http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf - Nov 2015)\n",
    "\n",
    "The whitepaper gave the following examples of how Google has been using Dremel since 2006:\n",
    "\n",
    ">* Analysis of crawled web documents.\n",
    ">* Tracking install data for applications on Android Market.\n",
    ">* Crash reporting for Google products.\n",
    ">* OCR results from Google Books.\n",
    ">* Spam analysis.\n",
    ">* Debugging of map tiles on Google Maps.\n",
    ">* Tablet migrations in managed Bigtable instances.\n",
    ">* Results of tests run on Google’s distributed build system.\n",
    ">* Disk I/O statistics for hundreds of thousands of disks.\n",
    ">* Resource monitoring for jobs run in Google’s data centers.\n",
    ">* Symbols and dependencies in Google’s codebase.\n",
    "\n",
    "At the high level, by focusing on a read-only and columnar data structure instead of a traditional relational database, Google was able to achieve high scale and good interactive performance. Compared to MapReduce models which are batch based, the technology behind Dremel could enable an interactive data science workflow.   \n",
    "\n",
    "Following the Amazon model of turning internal technologies into PaaS offerings, Google launched BigQuery as a product in 2010. They describe the product as:\n",
    "\n",
    ">BigQuery is Google's fully managed, NoOps, low cost data analytics service. With BigQuery you have no infrastructure to manage and don't need a database administrator, use familiar SQL and can take advantage of pay-as-you-go model. This collection of features allows you to focus on analyzing data to find meaningful insights. BigQuery is a powerful Big Data analytics platform used by all types of organizations, from startups to Fortune 500 companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIgning up for Free Google BigQuery Trial\n",
    "\n",
    "One of the advantages of BigQuery for data science is that it has a web UI, so you can explore data from a web interface instead of having to use a SQL client or API. In our case, this will allow you to test your SQL result prior to reading the results into Python for more detailed analysis.\n",
    "\n",
    "For our class, BigQuery is also convenient in that Google has several different publicly datasets that you can access once you set up an account. These public datasets are maintained by Google's Felipe Hoffa details of which are here:\n",
    "\n",
    "* https://www.reddit.com/r/bigquery/wiki/datasets\n",
    "\n",
    "\n",
    "Google has a free trial that includes a $300 credit that can be used over 60 days. BigQuery also always has a free usage tier up to 1 TB of data processed per month. Per Google, they require your billing information to \"verify that you're a real person. Don't worry, you will not be billed for the free trial.\" \n",
    "\n",
    "Also, per their site:\n",
    "\n",
    ">Do I have to pay when my free trial ends?\n",
    ">\n",
    ">No. You're under no obligation to buy anything when the free trial ends. If you want to continue to use Google Cloud Platform, just upgrade before your trial runs out.\n",
    "\n",
    "https://console.developers.google.com/freetrial\n",
    "\n",
    "You can either sign up with your ASU Google account or a personal Google Account. To sign up from ASU, login to your ASU account, open the ASU GMail app, and visit this page which includes all of the relevant service details:\n",
    "\n",
    "https://cloud.google.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BigQuery UI\n",
    "\n",
    "Once you have signed up for the service and area logged into the BigQuery console, you can add the public datasets to your console by pasting their URLs into your browser.  For example, paste or click on the following links after you are logged into the Google Cloud and they should be added to your available projects:\n",
    "\n",
    "* https://bigquery.cloud.google.com/table/bigquery-samples:reddit.full\n",
    "* https://bigquery.cloud.google.com/dataset/imjasonh-storage:nfl\n",
    "\n",
    "The console should look like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/week3-gbq_console.png\">\n",
    "\n",
    "From the console, you can select a dataset from the left nav which will then make the query box active. In the example below, I have selected the [GSOD dataset which contains weather data from NOAA](https://data.noaa.gov/dataset/global-surface-summary-of-the-day-gsod). Under the query box, there are options to view the table schema and details of the dataset.\n",
    "\n",
    "The query screen looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/azbones/big_data/master/images/week3-gbq_query.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Syntax\n",
    "\n",
    "Google's BigQuery products uses SQL-like syntax, but does not conform to ANSI SQL. Details of the syntax are included here:\n",
    "\n",
    "* https://cloud.google.com/bigquery/query-reference\n",
    "\n",
    "To get some experience in BigQuery, use the UI to derive the following metrics using the defined BigQuery command. For this exercise, we are not running code in the notebook, so just check your own work.\n",
    "\n",
    "Use the GSOD weather dataset:\n",
    "\n",
    "* https://bigquery.cloud.google.com/table/publicdata:samples.gsod\n",
    "\n",
    "**BigQuery Exercises**\n",
    "\n",
    "All of these statements begin with <code>SELECT</code> and build out criteria to define the selection.\n",
    "\n",
    "* Use <code>COUNT</code> and <code>GROUP BY</code> to calculate the number of observations in the year 1989\n",
    "\n",
    "* Use <code>WHERE</code> to return all of the observations with mean wind speeds greater than 75\n",
    "\n",
    "* Use <code>COUNT</code> and <code>WHERE</code> to return the number of observations with mean wind speeds greater than 75\n",
    "\n",
    "* Use <code>AVG</code>, <code>STDDEV</code>, and <code>AS</code> to calculate the average of the mean temperatures across all observations and to give the results a descriptive name\n",
    "\n",
    "* Use <code>AVG</code>, <code>STDDEV</code>, and <code>AS</code> to calculate the average of the mean temperatures across all observations and to give the results a descriptive name\n",
    "\n",
    "**BigQuery Table Organization**\n",
    "\n",
    "A common model for sending data to BigQuery splits tables into different time intervals. Querying across such intervals is easy as multiple tables with the same schema can be added to the <code>FROM</code> statement separated by a comma. For example, Google Analytics Premium customers can get raw, hit level data exported to BigQuery. The table naming convention for this appends the specific date of the extract to the table name like this:\n",
    "\n",
    "<code>dataset.ga_sessions_20151115</code>\n",
    "\n",
    "In addition to having a list of comma separated tables, BigQuery allows queries that define a time-based wildcard to retrieve specific date ranges without a scan of all tables like:\n",
    "\n",
    "````\n",
    "SELECT COUNT(*)\n",
    "FROM (TABLE_DATE_RANGE(dataset.ga_sessions_, \n",
    "      TIMESTAMP('2015-11-01'), \n",
    "      TIMESTAMP('2015-11-15'))) \n",
    "WHERE device.browser = \"Chrome\"\n",
    "````\n",
    "\n",
    "With ongoing data extracts that are time based, such organization can further optimize performance with multi-petabyte sized datasets given research questions may be limited by time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas and BigQuery\n",
    "\n",
    "The Python pandas library has a submodule which offers direct access to Google's BigQuery for read and write access called <code>gbq</code>. For desktop analysis, this submodule works by using [OAuth2.0](http://oauth.net/2/) to authenticate the desktop application as a valid user of the BigQuery account. Practically, you accomplish this by being logged into BigQuery in your default browser and then running the the <code>gbq</code> method.\n",
    "\n",
    "The basic read syntax follows:\n",
    "\n",
    "```\n",
    "projectid = \"xxxxxxxx\"\n",
    "\n",
    "df = pd.read_gbq(query='SELECT * FROM test_dataset.test_table', project_id=projectid)\n",
    "```\n",
    "\n",
    "When populated with a valid project id from the BigQuery console, the <code>read_gbq</code> method will open the default browser to an authentication page for BigQuery and, if successful, then save an access token to the local file system to provide access in the future. Once access is granted, <code>read_gbq</code> will send the SQL command in parameter <code>query</code> to BigQuery and return the results. The method's parameters include:\n",
    "\n",
    "\n",
    "Parameters:\t\n",
    "* query : str : SQL-Like Query to return data values\n",
    "* project_id : str : Google BigQuery Account project ID.\n",
    "* index_col : str (optional) : Name of result column to use for index in results DataFrame\n",
    "* col_order : list(str) (optional) : List of BigQuery column names in the desired order for results DataFrame\n",
    "* reauth : boolean (default False) : Force Google BigQuery to reauthenticate the user. This is useful if multiple accounts are used.\n",
    "* verbose : boolean (default True) : Verbose output\n",
    "\n",
    "An example of code and the output from a first authentication run of the code is in the code and markdown blocks below. Note that the <code>%time</code> magic command entered before a statement in IPython returns various time information related to its execution. This information can be useful especially when calling an external API like BigQuery to measure performance.\n",
    "\n",
    "\n",
    "> Note: if you are using control characters like quotation marks within a SQL query, you may need to add a \"\\\" to escape them so that Python can understand they are within the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io import gbq\n",
    "\n",
    "query = \"\"\"SELECT count(*) as num_records\n",
    "           FROM publicdata:samples.gsod\"\"\"\n",
    "\n",
    "project_id = xxxxxxx # Insert your project id from https://console.developers.google.com/ here\n",
    "\n",
    "%time gsod_year = gbq.read_gbq(query=query, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output From First Run:\n",
    "\n",
    "```\n",
    "Your browser has been opened to visit:\n",
    "\n",
    "    https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&response_type=code&client_id=XXXXXX.apps.googleusercontent.com&access_type=offline\n",
    "\n",
    "If your browser is on a different machine then exit and re-run this\n",
    "application with the command-line parameter \n",
    "\n",
    "  --noauth_local_webserver\n",
    "\n",
    "Authentication successful.\n",
    "Job not yet complete...\n",
    "CPU times: user 183 ms, sys: 1.36 s, total: 1.54 s\n",
    "Wall time: 14.4 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print your dataframe from gbq\n",
    "gsod_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Operations\n",
    "\n",
    "Before we move to the next exercise, we are going to review some more pandas dataframe operations. For these examples, you can use BigQuery to return some data for analysis or use any other datasets you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive Statistics**\n",
    "\n",
    "pandas offers a variety of descriptive statistics methods that can be called on series or dataframes. Let's read the iris dataset from week 1 into a dataframe. Note that in the example below we are going to read all five columns into the dataframe rather than use the class as the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame.from_csv('../week_1/datasets/iris_data.csv', index_col=None) # 5 columns, not using class as index\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the <code>describe</code> method to generate a range of statistics for the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the whole dataframe.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select just the 'Volume' column and <code>describe</code> it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the sepal_width column only.\n",
    "df['sepal_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are available for dataframes and series in pandas. Try them out in the next code block.\n",
    "\n",
    "```\n",
    "Function\tDescription\n",
    "--------    -----------\n",
    "count\t   Number of non-null observations\n",
    "sum\t     Sum of values\n",
    "mean\t    Mean of values\n",
    "mad\t     Mean absolute deviation\n",
    "median      Arithmetic median of values\n",
    "min\t     Minimum\n",
    "max\t     Maximum\n",
    "mode\t    Mode\n",
    "abs\t     Absolute Value\n",
    "prod\t    Product of values\n",
    "std\t     Unbiased standard deviation\n",
    "var\t     Unbiased variance\n",
    "sem\t     Unbiased standard error of the mean\n",
    "skew\t    Unbiased skewness (3rd moment)\n",
    "kurt\t    Unbiased kurtosis (4th moment)\n",
    "quantile\tSample quantile (value at %)\n",
    "cumsum\t  Cumulative sum\n",
    "cumprod\t Cumulative product\n",
    "cummax\t  Cumulative maximum\n",
    "cummin\t  Cumulative minimum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try out more statistical functions.\n",
    "df['sepal_width'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group By**\n",
    "\n",
    "The pandas <code>groupby</code> function provides a powerful tool to collect groups of dataframe elements together to perform functions on those groups independently. For example, we can use the 'class' column to create a groupby object to get statistics aggregated by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a groupby object.\n",
    "df_grp = df.groupby('class')\n",
    "\n",
    "# Calculate the average volume by type.\n",
    "df_grp['sepal_length'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate some other statistics for all the measures using the <code>groupby</code> object in the codeblock below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate more statistics.\n",
    "df_grp['sepal_width'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grp['sepal_width'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot and Pivot Table**\n",
    "\n",
    "The <code>pivot</code> and <code>pivot_table</code> functions are quite useful for reshaping data for analysis. While most analyses will be done with denormalized data in a case/wide format, data may arrive in a stacked or record format. \n",
    "\n",
    "**Pivot**\n",
    "\n",
    "The more basic <code>pivot</code> function creates a new dataframe using one of the columns to generate values, another to define the columns, and a third to define the values in the array. The three parameters of the <code>pivot</code> function are:\n",
    "\n",
    "```\n",
    "index (string or object, optional): Column name to use to make new frame’s index. If None, uses existing index.\n",
    "columns (string or object): Column name to use to make new frame’s columns\n",
    "values (string or object, optional): Column name to use for populating new frame’s values\n",
    "```\n",
    "Let's make a stacked dataframe as an example in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a stacked dataframe\n",
    "super_df = pd.DataFrame({'universe' : ['dc','dc','dc','dc','dc','dc','marvel','marvel','marvel'],\n",
    "                         'name' : ['Batman','Batman','Batman','Harley Quinn','Harley Quinn',\n",
    "                                   'Harley Quinn','Mockingbird','Mockingbird','Mockingbird'],\n",
    "                         'attributes' : ['Strength', 'Dexterity', 'Intellegence', 'Strength', 'Dexterity', \n",
    "                                         'Intellegence','Strength', 'Dexterity', 'Intellegence'],\n",
    "                         'score' : np.random.randint(low=0,high=100, size=(9))}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the dataframe\n",
    "super_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <code>pivot</code> function below to create a pivot with the names for the index, attribute type for the columns, and score for the values.\n",
    "\n",
    "Details on <code>pivot</code> are here:\n",
    "\n",
    "* http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.pivot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pivot to shape the data into a wide format and print output\n",
    "super_df.pivot(index='name', columns='attributes', values='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot Tables**\n",
    "\n",
    "Pivot tables are well known to most users of Microsoft Excel. Like a pivot, a pivot table translates the format of a dataframe, but also can aggregate values from the individual records. \n",
    "\n",
    "Details can be found here:\n",
    "\n",
    "* http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.pivot_table.html\n",
    "\n",
    "To get a view of average attributes by universe, use <code>pivot_table</code> on the dataframe to make the universe the index, the columns the attributes, and the values the score.  Use the aggfunc <code>'mean'</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pivot_table to shape the data into a wide format and print output\n",
    "super_df.pivot_table(index='universe', columns='attributes', values='score', aggfunc=np.mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
